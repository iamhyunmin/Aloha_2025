import pandas as pd
import numpy as np
import faiss, os, re, math
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ----------------------------
# ì„¤ì •
# ----------------------------
IN_PATH  = "data/store_with_rag_text.csv"   # ë¬¸ì¥í™” ì™„ë£Œëœ CSV
OUT_DIR  = "artifacts"                 # ì¸ë±ìŠ¤ ì €ì¥ í´ë”
EMB_MODEL = "BAAI/bge-m3"              # ì„ë² ë”© ëª¨ë¸
CHUNK_SIZE = 700                       # ì²­í‚¹ í¬ê¸° (í† í° ë‹¨ìœ„ ê·¼ì‚¬)
CHUNK_OVERLAP = 100                    # ì²­í‚¹ ì˜¤ë²„ë©
TOP_N = 5                              # ê²€ìƒ‰ ì‹œ top-k ê¸°ë³¸ê°’

os.makedirs(OUT_DIR, exist_ok=True)

# ----------------------------
# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ
# ----------------------------
print(f"[INFO] Loading data from {IN_PATH} ...")
df = pd.read_csv(IN_PATH, encoding="utf-8-sig")
df = df[df["rag_text"].notna() & (df["rag_text"].str.strip() != "")]
print(f"[INFO] Rows loaded: {len(df):,}")

# ----------------------------
# 2ï¸âƒ£ ì²­í‚¹ (Chunking)
# ----------------------------
print("[INFO] Splitting text into chunks ...")
splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=["\n", ".", " "]
)

chunks = []
for _, row in tqdm(df.iterrows(), total=len(df)):
    text = str(row["rag_text"])
    parts = splitter.split_text(text)
    for p in parts:
        if len(p.strip()) < 30:
            continue
        chunks.append({
            "ENCODED_MCT": row.get("ENCODED_MCT", ""),
            "rag_text": p.strip(),
            "TA_YM": row.get("TA_YM", ""),
            "HPSN_MCT_ZCD_NM": row.get("HPSN_MCT_ZCD_NM", ""),
            "MCT_SIGUNGU_NM": row.get("MCT_SIGUNGU_NM", "")
        })

meta = pd.DataFrame(chunks)
print(f"[INFO] Total chunks generated: {len(meta):,}")

# ----------------------------
# 3ï¸âƒ£ ì„ë² ë”© (Embedding)
# ----------------------------
print(f"[INFO] Loading embedding model: {EMB_MODEL}")
model = SentenceTransformer(EMB_MODEL, device="cuda")

texts = meta["rag_text"].tolist()
print(f"[INFO] Encoding {len(texts):,} chunks ...")
embs = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)
embs = np.array(embs, dtype="float32")

# ----------------------------
# 4ï¸âƒ£ íŒŒì‹œìŠ¤ ì¸ë±ì‹± (FAISS Index)
# ----------------------------
print("[INFO] Building FAISS index ...")
dim = embs.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(embs)
print(f"[INFO] FAISS index built: dim={dim}, vectors={index.ntotal:,}")

# ----------------------------
# 5ï¸âƒ£ ì €ì¥ (Index, Metadata, Keys)
# ----------------------------
index_path = os.path.join(OUT_DIR, "rag_faiss.index")
meta_path  = os.path.join(OUT_DIR, "meta.csv")
keys_path  = os.path.join(OUT_DIR, "document_keys.npy")

faiss.write_index(index, index_path)
meta.to_csv(meta_path, index=False, encoding="utf-8-sig")
np.save(keys_path, meta["ENCODED_MCT"].to_numpy())

print(f"[DONE] All artifacts saved to '{OUT_DIR}'")
print(f"        â€¢ Index: {index_path}")
print(f"        â€¢ Metadata: {meta_path}")
print(f"        â€¢ Keys: {keys_path}")

# ----------------------------
# 6ï¸âƒ£ ê²€ìƒ‰ í•¨ìˆ˜ ì˜ˆì‹œ (í…ŒìŠ¤íŠ¸ìš©)
# ----------------------------
def search(query, top_k=TOP_N):
    q_emb = model.encode([query], normalize_embeddings=True)
    D, I = index.search(np.array(q_emb, dtype="float32"), top_k)
    results = meta.iloc[I[0]].copy()
    results["score"] = D[0]
    return results[["ENCODED_MCT", "MCT_SIGUNGU_NM", "HPSN_MCT_ZCD_NM", "score", "rag_text"]]

if __name__ == "__main__":
    # ì˜ˆì‹œ ì§ˆì˜ í…ŒìŠ¤íŠ¸
    q = "ì„±ë™êµ¬ ì¹´í˜ ë§¤ì¶œì´ ë†’ì€ ë§¤ì¥"
    print("\n[TEST SEARCH]", q)
    res = search(q, top_k=3)
    for _, r in res.iterrows():
        print(f"ğŸª {r['MCT_SIGUNGU_NM']} | {r['HPSN_MCT_ZCD_NM']} | score={r['score']:.3f}")
        print(f"â†’ {r['rag_text'][:120]}...\n")
